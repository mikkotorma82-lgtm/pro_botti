import os, sys, time, argparse, pathlib, tempfile, json
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional
import pandas as pd

# ---- Project root to sys.path (so "history" package resolves) ----
ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from history.history_utils import DEFAULT_TARGET_DAYS, tf_to_seconds

import ccxt
import yfinance as yf

DATA_DIR = os.environ.get("DATA_DIR", "data")
HIST_DIR = os.path.join(DATA_DIR, "history")
os.makedirs(HIST_DIR, exist_ok=True)

YF_MAP = {
    "EURUSD": "EURUSD=X",
    "GBPUSD": "GBPUSD=X",
    "US500": "^GSPC",
    "US100": "^NDX",
    "TSLA": "TSLA",
    "AAPL": "AAPL",
    "NVDA": "NVDA",
}
YF_CRYPTO_FALLBACK = {
    "BTCUSDT": "BTC-USD",
    "ETHUSDT": "ETH-USD",
    "XRPUSDT": "XRP-USD",
    "SOLUSDT": "SOL-USD",
    "ADAUSDT": "ADA-USD",
}
BINANCE_TF = {"15m": "15m", "1h": "1h", "4h": "4h"}

# ---------- helpers ----------

def is_crypto(symbol: str) -> bool:
    return symbol.upper().endswith("USDT")

def to_binance_market(symbol: str) -> str:
    s = symbol.upper()
    if s.endswith("USDT"):
        base = s[:-4]
        return f"{base}/USDT"
    return s

def clamp_days_for_yf(tf: str, target_days: int) -> int:
    # Yahoo intraday -rajat: vältetään turhat callit
    if tf == "15m":
        return min(target_days, 60)
    if tf == "1h":
        return min(target_days, 730)
    if tf == "4h":
        return min(target_days, 3650)
    return target_days

def csv_path(symbol: str, tf: str) -> str:
    return os.path.join(HIST_DIR, f"{symbol}_{tf}.csv")

def lock_path(symbol: str, tf: str) -> str:
    return os.path.join(HIST_DIR, f"{symbol}_{tf}.lock")

def acquire_lock(lp: str, stale_seconds: int = 2*60*60) -> Optional[int]:
    """Return fd if acquired, else None. Removes stale locks."""
    try:
        if os.path.exists(lp):
            # stale?
            mtime = os.path.getmtime(lp)
            if time.time() - mtime > stale_seconds:
                try:
                    os.remove(lp)
                except Exception:
                    pass
        fd = os.open(lp, os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0o644)
        os.write(fd, str(os.getpid()).encode())
        return fd
    except FileExistsError:
        return None

def release_lock(fd: Optional[int], lp: str):
    try:
        if fd is not None:
            os.close(fd)
        if os.path.exists(lp):
            os.remove(lp)
    except Exception:
        pass

def read_existing_info(path: str) -> tuple[Optional[int], int]:
    """Return (last_ts, rows). last_ts in epoch seconds or None if missing/empty."""
    if not os.path.exists(path):
        return None, 0
    try:
        df = pd.read_csv(path); $'DROP_TIME=["time","date","datetime","timestamp","open_time","close_time"]
df.drop(columns=[c for c in DROP_TIME if c in df.columns], inplace=True, errors="ignore")
df = df.select_dtypes(include="number")'
        if df.empty or "time" not in df.columns:
            return None, 0
        last_ts = int(df.iloc[-1]["time"])
        return last_ts, len(df)
    except Exception:
        return None, 0

def atomic_write_df(df: pd.DataFrame, out_path: str):
    tmp_fd, tmp_name = tempfile.mkstemp(prefix=".hist_", dir=HIST_DIR)
    os.close(tmp_fd)
    df.to_csv(tmp_name, index=False)
    os.replace(tmp_name, out_path)

def log(msg: str):
    print(msg, flush=True)

# ---------- sources ----------

def fetch_crypto_binance_append(symbol: str, tf: str, days: int, start_from_ts: Optional[int]) -> List[Tuple[int,float,float,float,float,float]]:
    ex = ccxt.binance()
    market = to_binance_market(symbol)
    limit = 1000
    if start_from_ts is not None:
        since_ms = (start_from_ts + tf_to_seconds(tf)) * 1000
    else:
        since_ms = int((datetime.now(timezone.utc) - timedelta(days=days)).timestamp()*1000)
    out: List[List[float]] = []
    timeframe = BINANCE_TF[tf]
    while True:
        batch = ex.fetch_ohlcv(market, timeframe=timeframe, since=since_ms, limit=limit)
        if not batch:
            break
        out.extend(batch)
        if len(batch) < limit:
            break
        since_ms = batch[-1][0] + tf_to_seconds(tf)*1000
        if len(out) > 1_500_000:
            break
        time.sleep(0.05)
    return [(int(t/1000), float(o), float(h), float(l), float(c), float(v)) for (t,o,h,l,c,v) in out]

def fetch_yf_block(symbol: str, tf: str, days: int) -> List[Tuple[int,float,float,float,float,float]]:
    yf_symbol = YF_MAP.get(symbol, YF_CRYPTO_FALLBACK.get(symbol, symbol))
    period = f"{days}d"
    interval = {"15m":"15m", "1h":"60m", "4h":"240m"}[tf]
    try:
        df = yf.download(yf_symbol, period=period, interval=interval, auto_adjust=False, progress=False)
    except Exception as e:
        log(f"[WARN] yfinance fail {symbol} {tf}: {e}")
        return []
    if df is None or df.empty:
        return []
    df = df.rename(columns={"Open":"open","High":"high","Low":"low","Close":"close","Volume":"volume"})
    df = df.dropna(subset=["open","high","low","close"])
    rows: List[Tuple[int,float,float,float,float,float]] = []
    for idx, row in df.iterrows():
        ts = int(pd.Timestamp(idx).tz_localize(None).timestamp())
        rows.append((ts, float(row["open"]), float(row["high"]), float(row["low"]), float(row["close"]), float(row.get("volume", 0.0) or 0.0)))
    return rows

# ---------- main fetch logic ----------

def process_symbol_tf(symbol: str, tf: str, target_days: int, force: bool, update: bool, max_age_min: int) -> bool:
    out_csv = csv_path(symbol, tf)
    lock = lock_path(symbol, tf)
    lfd = acquire_lock(lock)
    if lfd is None:
        log(f"[SKIP] {symbol} {tf}: toinen prosessi tekee tätä (lock).")
        return True
    try:
        last_ts, rows_existing = read_existing_info(out_csv)
        # tuoreustarkistus
        if not force and last_ts is not None:
            age_sec = int(time.time()) - last_ts
            if age_sec < max_age_min * 60:
                log(f"[SKIP] {symbol} {tf}: tuore (age {age_sec//60} min < {max_age_min} min).")
                return True

        if not force and not update and rows_existing > 0:
            log(f"[SKIP] {symbol} {tf}: tiedosto on jo olemassa (update=off).")
            return True

        # hae uudet rivit
        if is_crypto(symbol):
            rows = fetch_crypto_binance_append(symbol, tf, target_days, start_from_ts=last_ts if update else None)
            mode = "APPEND" if last_ts is not None and rows else "WRITE"
        else:
            # Yahoo: vältetään turhaa uudelleenlatausta: jos tiedosto on, ja ei vanha -> skipattiin jo yllä.
            # Jos vanha -> haetaan koko blokki rajatulla periodilla ja yhdistetään deduplikoiden.
            rows = fetch_yf_block(symbol, tf, clamp_days_for_yf(tf, target_days))
            mode = "MERGE" if last_ts is not None and rows_existing > 0 else "WRITE"

        if not rows:
            log(f"[NOTE] {symbol} {tf}: ei uusia rivejä (mode={mode}).")
            return rows_existing > 0  # ok jos meillä on jo dataa

        new_df = pd.DataFrame(rows, columns=["time","open","high","low","close","volume"])

        if os.path.exists(out_csv):
            try:
                old_df = pd.read_csv(out_csv); $'DROP_TIME=["time","date","datetime","timestamp","open_time","close_time"]
df.drop(columns=[c for c in DROP_TIME if c in df.columns], inplace=True, errors="ignore")
df = df.select_dtypes(include="number")'
            except Exception:
                old_df = pd.DataFrame(columns=["time","open","high","low","close","volume"])
        else:
            old_df = pd.DataFrame(columns=["time","open","high","low","close","volume"])

        # yhdistä ja deduplikaa
        all_df = pd.concat([old_df, new_df], ignore_index=True)
        all_df = all_df.drop_duplicates(subset=["time"], keep="last")
        all_df = all_df.sort_values("time").reset_index(drop=True)

        atomic_write_df(all_df, out_csv)
        log(f"[OK] {symbol} {tf}: {mode} rows_old={len(old_df)} + rows_new={len(new_df)} => rows_now={len(all_df)}")
        return True
    finally:
        release_lock(lfd, lock)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbols", nargs="*", default=[
        "BTCUSDT","ETHUSDT","XRPUSDT","SOLUSDT","ADAUSDT",
        "EURUSD","GBPUSD","US500","US100","TSLA","AAPL","NVDA"
    ])
    ap.add_argument("--tfs", nargs="*", default=["15m","1h","4h"])
    ap.add_argument("--days-15m", type=int, default=DEFAULT_TARGET_DAYS["15m"])
    ap.add_argument("--days-1h",  type=int, default=DEFAULT_TARGET_DAYS["1h"])
    ap.add_argument("--days-4h",  type=int, default=DEFAULT_TARGET_DAYS["4h"])
    ap.add_argument("--force", action="store_true", help="Kirjoita uusiksi (ohita skip/tuoreus).")
    ap.add_argument("--no-update", dest="update", action="store_false", help="Älä appendaa; skip jos tiedosto on.")
    ap.add_argument("--max-age-min", type=int, default=60, help="Jos viimeinen kynttilä uudempi kuin tämä, skipataan (oletus 60 min).")
    args = ap.parse_args()

    tf_days = {"15m": args.days_15m, "1h": args.days_1h, "4h": args.days_4h}
    ok_all = True
    for s in args.symbols:
        for tf in args.tfs:
            ok = process_symbol_tf(s, tf, tf_days.get(tf, 365), force=args.force, update=args.update, max_age_min=args.max_age_min)
            ok_all = ok_all and ok
    if not ok_all:
        sys.exit(2)

if __name__ == "__main__":
    main()
