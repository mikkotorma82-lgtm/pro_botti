import argparse, json, yaml
from pathlib import Path
import numpy as np
import pandas as pd
from joblib import dump
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from tools.lr_safe import SafeLogistic as LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import classification_report

ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data" / "history"
MODEL_DIR = ROOT / "models"
REPORT_DIR = ROOT / "data" / "reports_v2"
MODEL_DIR.mkdir(parents=True, exist_ok=True)
REPORT_DIR.mkdir(parents=True, exist_ok=True)

# --- features ---
from core.features import make_features

def load_cfg(path: Path) -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)

def load_df(symbol: str, tf: str) -> pd.DataFrame:
    p = DATA_DIR / f"{symbol}_{tf}.csv"
    if not p.exists():
        raise FileNotFoundError(p)
    df = pd.read_csv(p); $'DROP_TIME=["time","date","datetime","timestamp","open_time","close_time"]
df.drop(columns=[c for c in DROP_TIME if c in df.columns], inplace=True, errors="ignore")
df = df.select_dtypes(include="number")'
    df = df.sort_values("time").reset_index(drop=True)
    for col in ["time","open","high","low","close","volume"]:
        if col not in df.columns:
            raise ValueError(f"{p} missing column {col}")
    return df

def walk_splits(n, n_splits=5):
    fold = n // (n_splits + 1)
    for k in range(n_splits):
        train_end = fold * (k+1)
        val_end = fold * (k+2)
        tr = np.arange(0, train_end)
        va = np.arange(train_end, min(val_end, n))
        if len(va) > 0:
            yield tr, va

def fit_one(symbol: str, tf: str, cfg: dict):
    df = load_df(symbol, tf)
    df_feat, feats_default = make_features(df)

    # features-valinta: config > default
    FEATS = cfg.get("features") or feats_default
    FEATS = [f for f in FEATS if f in df_feat.columns]
    if not FEATS:
        raise RuntimeError("Ei featureita treeniin")

    horizon = int(cfg.get("train", {}).get("horizon_bars", 1))
    fut = df_feat["close"].shift(-horizon) / df_feat["close"] - 1.0
    y = np.where(fut > 0.0, 1, np.where(fut < 0.0, -1, 0))
    mask = ~np.isnan(y)
    X = df_feat[FEATS].values[mask]
    y = y[mask].astype(int)
    n = len(y)
    if n < 200:
        raise RuntimeError(f"{symbol} {tf}: liian vähän dataa ({n})")

    # mallit
    lr = Pipeline([
        ("scaler", StandardScaler()),
        ("clf", LogisticRegression(max_iter=1000, C=1.0))
    ])
    rf = RandomForestClassifier(
        n_estimators=int(cfg.get("train", {}).get("rf_trees", 200)),
        max_depth=int(cfg.get("train", {}).get("rf_max_depth", 6)),
        n_jobs=-1,
        random_state=42,
    )
    clf = VotingClassifier(
        estimators=[("lr", lr), ("rf", rf)],
        voting="hard"
    )

    # rullaava CV
    reports = []
    n_splits = int(cfg.get("train", {}).get("n_splits", 5))
    for tr, va in walk_splits(n, n_splits):
        Xtr, ytr = X[tr], y[tr]
        Xva, yva = X[va], y[va]
        clf.fit(Xtr, ytr)
        yhat = clf.predict(Xva)
        rep = classification_report(yva, yhat, output_dict=True, zero_division=0)
        reports.append(rep)

    # treenaa lopullinen malli (viimeiset 2*horizon sivuun)
    cut = max(0, n - 2*horizon)
    clf.fit(X[:cut], y[:cut])

    model_path = MODEL_DIR / f"pro_{symbol}_{tf}.joblib"
    meta_path = MODEL_DIR / f"pro_{symbol}_{tf}.json"
    meta = {
        "features": FEATS,
        "horizon": int(horizon),
        "symbol": symbol,
        "tf": tf,
        "n": int(n),
        "trainer": "v2",
    }
    dump({"pipeline": clf, "features": FEATS}, model_path)
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)
    print(f"[OK v2] saved {model_path.name} + {meta_path.name}")

    # raportti
    avg_acc = float(np.mean([r["accuracy"] for r in reports])) if reports else None
    out = {
        "symbol": symbol, "tf": tf, "n": int(n),
        "avg_cv_accuracy": avg_acc,
        "model_path": str(model_path),
        "meta_path": str(meta_path),
        "reports": reports[-1] if reports else None
    }
    rep_path = REPORT_DIR / f"train_v2_{symbol}_{tf}.json"
    with open(rep_path, "w") as f: json.dump(out, f, indent=2)
    print(f"[REP v2] {rep_path}")
    return out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default="config.yaml")
    ap.add_argument("--symbols", nargs="*", default=None)
    ap.add_argument("--tfs", nargs="*", default=None)
    args = ap.parse_args()

    cfg = load_cfg(Path(args.config))
    symbols = args.symbols or (cfg.get("market", {}) or {}).get("symbols", [])
    if not symbols:
        symbols = (cfg.get("live", {}) or {}).get("symbols", [])
    if not symbols:
        raise SystemExit("Ei symboleita (market.symbols tai --symbols)")

    tfs = args.tfs or (cfg.get("train", {}) or {}).get("timeframes", ["15m","1h","4h"])

    for s in symbols:
        for tf in tfs:
            try:
                fit_one(s, tf, cfg)
            except Exception as e:
                print(f"[FAIL v2] {s} {tf}: {e}")

if __name__ == "__main__":
    main()
