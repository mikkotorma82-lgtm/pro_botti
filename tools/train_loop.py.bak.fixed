import os, argparse, json, time, yaml, math
from pathlib import Path
import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from tools.lr_safe import SafeLogistic as LogisticRegression
from sklearn.metrics import classification_report
from joblib import dump
from tools import auto_tune

ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data" / "history"
MODEL_DIR = ROOT / "models"
REPORT_DIR = ROOT / "data" / "reports"
MODEL_DIR.mkdir(parents=True, exist_ok=True)
REPORT_DIR.mkdir(parents=True, exist_ok=True)

def load_cfg(path: Path) -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)

def load_df(symbol: str, tf: str) -> pd.DataFrame:
    p = DATA_DIR / f"{symbol}_{tf}.csv"
    if not p.exists():
        raise FileNotFoundError(p)
    df = pd.read_csv(p); $'DROP_TIME=["time","date","datetime","timestamp","open_time","close_time"]
df.drop(columns=[c for c in DROP_TIME if c in df.columns], inplace=True, errors="ignore")
df = df.select_dtypes(include="number")'
    df = df.sort_values("time").reset_index(drop=True)
    # odotetaan: time, open, high, low, close, volume
    for col in ["open","high","low","close","volume"]:
        if col not in df.columns:
            raise ValueError(f"{p} missing column {col}")
    return df

# --- indikaattorit (ei ulkoisia riippuvuuksia) ---
def ema(s, span):
    return s.ewm(span=span, adjust=False).mean()

def rsi(close, period=14):
    delta = close.diff()
    up = (delta.clip(lower=0)).ewm(alpha=1/period, adjust=False).mean()
    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean()
    rs = up / (down + 1e-12)
    return 100 - (100 / (1 + rs))

def atr(df, period=14):
    hl = df["high"] - df["low"]
    hc = (df["high"] - df["close"].shift()).abs()
    lc = (df["low"] - df["close"].shift()).abs()
    tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)
    return tr.ewm(alpha=1/period, adjust=False).mean()

def make_features(df: pd.DataFrame) -> pd.DataFrame:
    z = df.copy()
    z["ret1"] = z["close"].pct_change()
    z["ret5"] = z["close"].pct_change(5)
    z["vol5"] = z["ret1"].rolling(48, min_periods=12).std() # ”vola”-proksi
    z["ema12"] = ema(z["close"], 12)
    z["ema26"] = ema(z["close"], 26)
    z["macd"] = z["ema12"] - z["ema26"]
    z["rsi14"] = rsi(z["close"], 14)
    z["atr14"] = atr(z, 14) / (z["close"]+1e-12)
    # signaaleja/poikkeamaa:
    z["ema_gap"] = (z["close"] - z["ema12"]) / (z["ema12"] + 1e-12)
    z = z.dropna().reset_index(drop=True)
    feats = ["ret1","ret5","vol5","ema12","ema26","macd","rsi14","atr14","ema_gap"]
    return z, feats

def label_future(df: pd.DataFrame, horizon=1, thr_pos=0.0, thr_neg=0.0):
    # 3-luokkainen: 1=long, -1=short, 0=flat (tiukat rajat -> paljon treidejä)
    fut = df["close"].shift(-horizon) / df["close"] - 1.0
    y = np.where(fut > thr_pos, 1, np.where(fut < -thr_neg, -1, 0))
    return pd.Series(y, index=df.index)

def walk_splits(n, n_splits=5):
    # aikapohjaiset splitit
    fold = n // (n_splits + 1)
    for k in range(n_splits):
        train_end = fold * (k+1)
        val_end = fold * (k+2)
        if val_end <= train_end+10:  # suojaraja
            continue
        yield slice(0, train_end), slice(train_end, val_end)

def fit_one(symbol, tf, cfg):
    df_raw = load_df(symbol, tf)
    df_feat, FEATS = make_features(df_raw)
    # kohde
    horizon = int(cfg["train"].get("horizon_bars", 1))
    y = label_future(df_feat, horizon=horizon, thr_pos=0.0, thr_neg=0.0)
    X = df_feat[FEATS].values
    # vain rivit, joilla label valid (drop viimeiset)
    mask = ~np.isnan(y.values)
    X, y = X[mask], y.values[mask]
    n = len(y)
    if n < 100:
        raise RuntimeError(f"{symbol} {tf}: liian vähän dataa ({n})")

    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("clf", LogisticRegression(max_iter=1000, C=1.0, n_jobs=None, multi_class="auto"))
    ])

    reports = []
    for tr, va in walk_splits(n, int(cfg["train"].get("n_splits", 5))):
        Xtr, ytr = X[tr], y[tr]
        Xva, yva = X[va], y[va]
        pipe.fit(Xtr, ytr)
        yhat = pipe.predict(Xva)
        rep = classification_report(yva, yhat, output_dict=True, zero_division=0)
        reports.append(rep)

    # treenaa lopullinen malli koko datalla - viimeiset 2*horizon jätetään varalle
    cut = max(0, n - 2*horizon)
    pipe.fit(X[:cut], y[:cut])

# --- AUTO-TUNE (ei aikarajoja) ---
proba = pipe.predict_proba(df_feat[FEATS].values)
classes = pipe.named_steps["clf"].classes_
try:
    idx_long  = list(classes).index(1)
except ValueError:
    idx_long = None
try:
    idx_short = list(classes).index(-1)
except ValueError:
    idx_short = None
import numpy as np
if idx_long is None:  pl = np.zeros(len(df_feat))
else:                 pl = proba[:, idx_long]
if idx_short is None: ps = np.zeros(len(df_feat))
else:                 ps = proba[:, idx_short]

prices = df_feat["close"].values
best = auto_tune.grid_search(symbol, tf, pl, ps, prices)

tuned_thrL = float(best["thrL"])
tuned_thrS = float(best["thrS"])
tuned_sl   = float(best["sl"])
tuned_tp   = float(best["tp"])
tuned_gap  = float(best["gap"])
tuned_decay= float(best["prob_decay"])
        meta = {
            "prob_threshold_long": float(tuned_thrL),
            "prob_threshold_short": float(tuned_thrS),
        }

    model_path = MODEL_DIR / f"ml_{symbol}_{tf}.joblib"
    meta_path = MODEL_DIR / f"ml_{symbol}_{tf}.json"
    dump({"pipeline": pipe, "features": meta["features"]}, model_path)
    with open(meta_path, "w") as f: json.dump(meta, f, indent=2)
    print(f"[OK] saved {model_path.name} + {meta_path.name}")

    # pikaraportti
    avg_acc = float(np.mean([r["accuracy"] for r in reports])) if reports else None
    out = {
        "symbol": symbol, "tf": tf, "n": int(n),
        "avg_cv_accuracy": avg_acc,
        "model_path": str(model_path),
        "meta_path": str(meta_path)
    }
    rep_path = REPORT_DIR / f"train_{symbol}_{tf}.json"
    with open(rep_path, "w") as f: json.dump(out, f, indent=2)
    print(f"[REP] {rep_path}")
    return out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default="config.yaml")
    ap.add_argument("--symbols", nargs="*", default=None)
    ap.add_argument("--tfs", nargs="*", default=None)
    args = ap.parse_args()

    cfg = load_cfg(Path(args.config))
    # symbolit
    symbols = args.symbols
    if not symbols:
        symbols = (cfg.get("market", {}) or {}).get("symbols", [])
    if not symbols:
        symbols = (cfg.get("live", {}) or {}).get("symbols", [])
    if not symbols:
        raise SystemExit("Ei symboleita (market.symbols tai --symbols)")

    # timeframe:t
    tfs = args.tfs or (cfg.get("train", {}) or {}).get("timeframes", ["15m","1h","4h"])

    for s in symbols:
        for tf in tfs:
            try:
                fit_one(s, tf, cfg)
            except Exception as e:
                print(f"[FAIL] {s} {tf}: {e}")

if __name__ == "__main__":
    main()
